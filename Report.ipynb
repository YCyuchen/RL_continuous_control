{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradient(DDPG) Network for Continuous Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand DDPG\n",
    "DDPG use actor-critic approach based on the DPG algorithm(actor-critic stands for two network here)\n",
    "- actor network: specifies the current policy by deterministically mapping states to a specific action.\n",
    "- critic network: learned by using the Bellman equation as in Q-learning.  \n",
    "\n",
    "DDPG is an off policy alogtithm, which means DDPG collect the states in a replay buffer when the agent interacts with the environment. When training the agent, the buffer randomly yield a batch with respect to (st,at,rt,st+1). We calculate the Q_targets and Q_expected through actor network and critic network respectivly。 loss is calculate through the mean square difference of Q_targets and Q_expected. Then weights in critic network is update through backprobagation. actor loss is defined as:  \n",
    "`actor_loss = -self.critic_local(states, actions_pred).mean()` which means?? (sorry i don't understand it yet.)  \n",
    "noise is added when select the action, the benefit of noise usage is demonstrate in this paper:Control policy with autocorrelated noise in reinforcement learning for robotics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 128]           4,352\n",
      "            Linear-2                   [-1, 64]           8,256\n",
      "            Linear-3                    [-1, 4]             260\n",
      "================================================================\n",
      "Total params: 12,868\n",
      "Trainable params: 12,868\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.05\n",
      "Estimated Total Size (MB): 0.05\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuchen/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "from model import Actor,Critic\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "\n",
    "state_size = 33\n",
    "action_size = 4\n",
    "seed = 0\n",
    "model = Actor(state_size, action_size, seed)\n",
    "summary(model, input_size=(33,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 128]           4,352\n",
      "            Linear-2                   [-1, 64]           8,512\n",
      "            Linear-3                    [-1, 1]              65\n",
      "================================================================\n",
      "Total params: 12,929\n",
      "Trainable params: 12,929\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.05\n",
      "Estimated Total Size (MB): 0.05\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "state_size = 33\n",
    "action_size = 4\n",
    "seed = 0\n",
    "model = Critic(state_size, action_size, seed)\n",
    "summary(model, [(state_size,),(action_size,)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size  \n",
    "BATCH_SIZE = 128        # minibatch size  \n",
    "GAMMA = 0.99            # discount factor   \n",
    "TAU = 1e-3              # for soft update of target parameters   \n",
    "LR_ACTOR = 1e-4         # learning rate of the actor    \n",
    "LR_CRITIC = 1e-4        # learning rate of the critic    \n",
    "WEIGHT_DECAY = 0        # L2 weight decay   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### here we plot the episode-reward curve durning training\n",
    "![](https://raw.githubusercontent.com/YCyuchen/RL_continuous_control/master/score_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Futher work\n",
    "In this project, i complete the first version, which has a single agent. Futher work may consider completing the multiple agents envs to solve the environment.   \n",
    "Besides DDPG, other state of the art algroithm like  PPO, A3C, and D4PG deserved to try and see there performance on this env."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:\n",
    "In this paper, [Continuous control with deep reinorcement learning](Continuous control with deep reinforcement learning)  \n",
    "The author said The goal in reinforcement learning is to learn a policy which maximizes the expected return from the **start distribution** $$J = E_{ri,si∼E,ai∼π }[R1].$$\n",
    "my question is: \n",
    "- what's the difference between expected return and reward?(i still fell ambiguous about this two terminology )  \n",
    "**Reviewer**:The reward should be the actual reward given from the environment, whereas the expected return will likely be based on some estimation of rewards over a series of steps  \n",
    "**Update**:So, if i put expected return and reward in supervised learning task, the reward is very like the ground truth, and $expected return\\approx prediction$ and our goal is to make the expected return approximatly eaual to reward ? then select the action that can give the max reward?\n",
    "- how to understand the start distribution in above formula?  \n",
    "**Update**: Is the author means state distribution instead of start distribution? As i can't understand what is a state distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
